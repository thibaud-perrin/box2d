{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fb422d-6fb0-4159-b6e5-79bff41dc79b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bipedal Walker (hardcore)\n",
    "[Documentation](https://gymnasium.farama.org/environments/box2d/bipedal_walker/)  \n",
    "This environment is part of the Box2D environments which contains general information about the environment.\n",
    "\n",
    "[leaderboard](https://github.com/openai/gym/wiki/Leaderboard#bipedalwalker-v2)\n",
    "\n",
    "## Description\n",
    "This is a simple 4-joint walker robot environment. There are two versions:\n",
    "\n",
    "- Normal, with slightly uneven terrain.\n",
    "- Hardcore, with ladders, stumps, pitfalls.\n",
    "ls.\r\n",
    "\r\n",
    "To solve the normal version, you need to get 300 points in 1600 time steps. To solve the hardcore version, you need 300 points in 2000 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952bb491-c1c2-4d92-b0da-156f9025fa30",
   "metadata": {},
   "source": [
    "We have decided to used the DDPG algorithme with the TD3 variant, we used 5 critics and D2RL architecture model to help them to learn. We also use normalization on states and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28409768-9bc1-44c3-82ed-0ca9e0b7f251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import os\n",
    "import base64\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# import third-party libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "import imageio\n",
    "import cv2\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc43622-7836-473b-914d-6c143a7c5cb7",
   "metadata": {},
   "source": [
    "## Hardware infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fd67c7-c94a-49c2-90f7-e3ed1ea56ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1070 (UUID: GPU-19cef1c2-e216-e824-c98e-660394f8a4bb)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a786ed-9ed2-4f0b-ad4e-f1f22b4c0b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version.cuda=[11.7]\n",
      "torch.cuda.is_available(True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch.version.cuda=[{torch.version.cuda}]\")\n",
    "print(f\"torch.cuda.is_available({torch.cuda.is_available()})\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62e894b-412c-420a-bf10-7ede8eff4e55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232c832-cd45-432f-a602-4442a0105b43",
   "metadata": {},
   "source": [
    "## Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dc8a76-424c-4823-afb3-b04e7a116279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./img/bipedal_walker_hardcore :\n",
      "total 14004\n",
      "drwxr-xr-x 1 thiba 197609        0 Jun 26 09:34 .\n",
      "drwxr-xr-x 1 thiba 197609        0 Jun 26 09:31 ..\n",
      "-rw-r--r-- 1 thiba 197609 14334369 Jun 26 21:39 bipedal_walker_hardcore.gif\n",
      "./saves/bipedal_walker_hardcore :\n",
      "total 5033\n",
      "drwxr-xr-x 1 thiba 197609      0 Jun 26 16:49 .\n",
      "drwxr-xr-x 1 thiba 197609      0 Jun 26 09:31 ..\n",
      "drwxr-xr-x 1 thiba 197609      0 Jun 26 16:49 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 thiba 197609 854167 Jun 27 10:08 bipedal_walker_hardcore_actor_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 855215 Jun 27 10:08 bipedal_walker_hardcore_critic0_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 855215 Jun 27 10:08 bipedal_walker_hardcore_critic1_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 855215 Jun 27 08:21 bipedal_walker_hardcore_critic2_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 855215 Jun 27 08:21 bipedal_walker_hardcore_critic3_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 855215 Jun 27 08:21 bipedal_walker_hardcore_critic4_model.pth\n",
      "-rw-r--r-- 1 thiba 197609   1031 Jun 27 10:08 bipedal_walker_hardcore_normalizer.pkl\n",
      "-rw-r--r-- 1 thiba 197609    239 Jun 27 10:08 bipedal_walker_hardcore_reward_normalizer.pkl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('img/bipedal_walker_hardcore', exist_ok=True)\n",
    "os.makedirs('saves/bipedal_walker_hardcore', exist_ok=True)\n",
    "print('./img/bipedal_walker_hardcore :')\n",
    "!ls -al \"img/bipedal_walker_hardcore\"\n",
    "print('./saves/bipedal_walker_hardcore :')\n",
    "!ls -al \"saves/bipedal_walker_hardcore\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc8f7e-4ed4-4a9e-b222-f36d75a8efdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constants\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c447b469-f41b-48a4-98fd-bea9449be473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1_000_000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.98\n",
    "TAU = 0.02\n",
    "LR_ACTOR = 0.00005\n",
    "LR_CRITIC = 0.0002\n",
    "WEIGHT_DECAY = 0.00\n",
    "POLICY_NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "POLICY_FREQ = 2\n",
    "START_LEARNING = 10000 # steps in the beginning to let the agent explore\n",
    "N_TOTAL_EPISODES = 1000\n",
    "N_CRITICS = 2\n",
    "OPTIM = \"ADAM\" # \"LION\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fece19-c43a-45aa-95b1-b8a1a42ede8c",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c6b75d-0fe2-4209-ba7c-ceb424985e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./saves/bipedal_walker_hardcore/bipedal_walker_hardcore\"\n",
    "PATH_IMG = \"./img/bipedal_walker_hardcore/bipedal_walker_hardcore\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2e836-6d3c-411e-a629-f71c7ee5df96",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a31f47-dea9-4943-affa-2c3463dd42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_infos(ep_len_mean, ep_rew_mean, ep_len, ep_rew, min_rew, ep_min_rew, episodes, fps, time_elapsed, total_timesteps, actor_loss, critic_loss_arr, learning_rate, n_updates):\n",
    "    print(f\"- rollout/\")\n",
    "    print(f\"    - ep_len_mean     : {ep_len_mean}\")\n",
    "    print(f\"    - ep_rew_mean     : {ep_rew_mean}\")\n",
    "    print(f\"    - ep_len          : {ep_len}\")\n",
    "    print(f\"    - ep_rew          : {ep_rew}\")\n",
    "    print(f\"    - max_rew         : {min_rew}\")\n",
    "    print(f\"    - ind_max_rew     : {ep_min_rew}\")\n",
    "    print(f\"\")\n",
    "    print(f\"- time/\")\n",
    "    print(f\"    - episodes        : {episodes}\")\n",
    "    print(f\"    - fps             : {fps}\")\n",
    "    print(f\"    - time_elapsed    : {time_elapsed}\")\n",
    "    print(f\"    - total_timesteps : {total_timesteps}\")\n",
    "    print(f\"\")\n",
    "    print(f\"- train/\")\n",
    "    print(f\"    - actor_loss      : {actor_loss}\")\n",
    "    for i, c in enumerate(critic_loss_arr):\n",
    "        print(f\"    - critic_loss{i+1}    : {c}\")\n",
    "    print(f\"    - learning_rate   : {learning_rate}\")\n",
    "    print(f\"    - n_updates       : {n_updates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f0e68e6-212f-4df7-b36b-fe425b1a0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gif(img_list, path):\n",
    "    # Convert the list of frames to a numpy array\n",
    "    resized_img_array = []\n",
    "    for img in img_list:\n",
    "        img_pil = Image.fromarray(img)\n",
    "        # Make sure width and height are divisible by 16\n",
    "        img_resized_pil = img_pil.resize((608, 400))\n",
    "        img_resized = np.array(img_resized_pil)\n",
    "        resized_img_array.append(img_resized)\n",
    "    \n",
    "    # Create gif video\n",
    "    fps = 20\n",
    "    imageio.mimsave(path, resized_img_array, 'GIF', duration=int(1000 * 1/fps), loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b22232-a375-4c23-ba54-c7a818c19eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(reward_history, rolling_window=20, x_label = 'Episode', y_label = 'Total Points', lower_limit=None, upper_limit=None, plot_rw=True, plot_rm=True):\n",
    "    \"\"\"\n",
    "    Function to plot reward history and its rolling mean with some optional arguments.\n",
    "\n",
    "    Args:\n",
    "        reward_history (list): A list of rewards for each episode.\n",
    "        rolling_window (int): The number of episodes for computing the rolling mean.\n",
    "        lower_limit (int): Starting episode index for plotting.\n",
    "        upper_limit (int): Ending episode index for plotting.\n",
    "        plot_rw (bool): A flag for plotting raw reward history.\n",
    "        plot_rm (bool): A flag for plotting rolling mean reward history.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # If lower_limit and upper_limit are not provided, use the whole reward_history\n",
    "    if lower_limit is None or upper_limit is None:\n",
    "        rh = reward_history\n",
    "        xs = [x for x in range(len(reward_history))]\n",
    "    else:\n",
    "        rh = reward_history[lower_limit:upper_limit]\n",
    "        xs = [x for x in range(lower_limit,upper_limit)]\n",
    "   \n",
    "    # Create a DataFrame and calculate the rolling mean\n",
    "    df = pd.DataFrame(rh)\n",
    "    rollingMean = df.rolling(rolling_window).mean()\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10,7), facecolor='white')\n",
    "    \n",
    "    if plot_rw:\n",
    "        plt.plot(xs, rh, linewidth=1, color='cyan')\n",
    "    if plot_rm:\n",
    "        plt.plot(xs, rollingMean, linewidth=2, color='magenta')\n",
    "\n",
    "    text_color = 'black'\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('black')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.xlabel(x_label, color=text_color, fontsize=30)\n",
    "    plt.ylabel(y_label, color=text_color, fontsize=30)\n",
    "    yNumFmt = mticker.StrMethodFormatter('{x:,}')\n",
    "    ax.yaxis.set_major_formatter(yNumFmt)\n",
    "    ax.tick_params(axis='x', colors=text_color)\n",
    "    ax.tick_params(axis='y', colors=text_color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa23f826-b9fd-4a24-bc04-f587bd7c4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hyperparameters():\n",
    "    print(\"----------------------------------\")\n",
    "    print(f\"| {BUFFER_SIZE=} \\t\\t |\")\n",
    "    print(f\"| {BATCH_SIZE=} \\t\\t |\")\n",
    "    print(f\"| {GAMMA=} \\t\\t\\t |\")\n",
    "    print(f\"| {TAU=} \\t\\t\\t |\")\n",
    "    print(f\"| {LR_ACTOR=} \\t\\t |\")\n",
    "    print(f\"| {LR_CRITIC=} \\t\\t |\")\n",
    "    print(f\"| {WEIGHT_DECAY=} \\t\\t |\")\n",
    "    print(f\"| {POLICY_NOISE=} \\t\\t |\")\n",
    "    print(f\"| {NOISE_CLIP=} \\t\\t |\")\n",
    "    print(f\"| {POLICY_FREQ=} \\t\\t |\")\n",
    "    print(f\"| {START_LEARNING=} \\t\\t |\")\n",
    "    print(f\"| {N_TOTAL_EPISODES=} \\t |\")\n",
    "    print(f\"| {N_CRITICS=} \\t\\t\\t |\")\n",
    "    print(f\"| {OPTIM=} \\t\\t\\t |\")\n",
    "    print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d16eb8-5c5f-4aed-9bdb-b3e6724da8c9",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb680d6e-63ed-46d6-a891-cf2588682ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a453c0e-37c9-4aea-b294-30527387c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838f137-bd48-4670-b25c-cc0e90330274",
   "metadata": {},
   "source": [
    "## Normalizer\n",
    "### States normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28650759-d2b7-413d-81e7-0ae8d5c698ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    def __init__(self, size):\n",
    "        self.n = np.zeros(size)\n",
    "        self.mean = np.zeros(size)\n",
    "        self.M2 = np.zeros(size)\n",
    "        self.var = np.full(size, 1e-2)\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1.0\n",
    "        delta = x - self.mean\n",
    "        self.mean += delta / self.n\n",
    "        delta2 = x - self.mean\n",
    "        self.M2 += delta * delta2\n",
    "        less_than_two = self.n < 2\n",
    "        self.var = np.where(less_than_two, np.full(self.var.shape, 1e-2), self.M2 / (self.n - 1))\n",
    "        self.var = np.maximum(self.var, 1e-2)  # Clip the variance to a minimum value\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - self.mean) / obs_std\n",
    "\n",
    "    def observe2d(self, x):\n",
    "        x = np.atleast_2d(x)  # Make sure x is at least 2-D\n",
    "        batch_size, _ = x.shape\n",
    "        self.n += batch_size\n",
    "        for instance in x:\n",
    "            delta = instance - self.mean\n",
    "            self.mean += delta / self.n\n",
    "            delta2 = instance - self.mean\n",
    "            self.M2 += delta * delta2\n",
    "        less_than_two = self.n < 2\n",
    "        self.var = np.where(less_than_two, np.full(self.var.shape, 1e-2), self.M2 / (self.n - 1))\n",
    "        self.var = np.maximum(self.var, 1e-2)  # Clip the variance to a minimum value\n",
    "\n",
    "    def save(self):\n",
    "        with open(f\"{PATH}_normalizer.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "\n",
    "    def load(self):\n",
    "        with open(f\"{PATH}_normalizer.pkl\", 'rb') as f:\n",
    "            tmp_dict = pickle.load(f)\n",
    "        self.__dict__.update(tmp_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdc7cc-2c7e-4be6-ad87-91425329ae60",
   "metadata": {},
   "source": [
    "### Reward normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c959ac55-5f34-4798-a9be-8bea38bcf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNormalizer():\n",
    "    def __init__(self, dim=1):\n",
    "        self.n = 0\n",
    "        self.dim = dim\n",
    "        self.mean = np.zeros(self.dim)\n",
    "        self.M2 = np.zeros(self.dim)\n",
    "        self.var = np.full(self.dim, 1e-2)\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1\n",
    "        delta = x - self.mean\n",
    "        self.mean += delta / self.n\n",
    "        delta2 = x - self.mean\n",
    "        self.M2 += delta * delta2\n",
    "        if self.n < 2:\n",
    "            self.var = np.full(self.dim, 1e-2)\n",
    "        else:\n",
    "            self.var = self.M2 / (self.n - 1)\n",
    "        self.var = np.maximum(self.var, 1e-2)  # Clip the variance to a minimum value\n",
    "\n",
    "    def normalize(self, reward):\n",
    "        reward_std = np.sqrt(self.var)\n",
    "        return (reward - self.mean) / reward_std\n",
    "\n",
    "    def observe2d(self, x):\n",
    "        x = np.atleast_2d(x)  # Make sure x is at least 2-D\n",
    "        batch_size, dim = x.shape\n",
    "        assert dim == self.dim, \"Input dimensions do not match initialized dimensions\"\n",
    "        self.n += batch_size\n",
    "        for instance in x:\n",
    "            delta = instance - self.mean\n",
    "            self.mean += delta / self.n\n",
    "            delta2 = instance - self.mean\n",
    "            self.M2 += delta * delta2\n",
    "        if self.n < 2:\n",
    "            self.var = np.full(self.dim, 1e-2)\n",
    "        else:\n",
    "            self.var = self.M2 / (self.n - 1)\n",
    "        self.var = np.maximum(self.var, 1e-2)  # Clip the variance to a minimum value\n",
    "\n",
    "    def save(self):\n",
    "        with open(f\"{PATH}_reward_normalizer.pkl\", 'wb') as f:\n",
    "            pickle.dump((self.n, self.mean, self.M2, self.var), f)\n",
    "\n",
    "    def load(self):\n",
    "        with open(f\"{PATH}_reward_normalizer.pkl\", 'rb') as f:\n",
    "            self.n, self.mean, self.M2, self.var = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d0fb2-6a15-47d0-b950-cda0633cd3d5",
   "metadata": {},
   "source": [
    "## Network\n",
    "### Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9310d32-699c-45a5-a79c-39dde680481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_ego_size, state_ext_size, action_size, seed, fc1_units=256, fc2_units=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fceg1 = nn.Linear(state_ego_size, fc1_units)                                     # egocentric information (14)\n",
    "        \n",
    "        self.fcex1 = nn.Linear(state_ext_size, fc1_units)                                     # exteroceptive information (10)\n",
    "        self.fcex2 = nn.Linear(fc1_units + state_ext_size, fc1_units + state_ext_size)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc1_units + fc1_units + state_ext_size, fc2_units)               # merge [egocentric + exteroceptive](24)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state_ego, state_ext):\n",
    "        x_ego = F.relu(self.fceg1(state_ego))\n",
    "        \n",
    "        x_ext1 = F.relu(self.fcex1(state_ext))\n",
    "        x_ext2 = F.relu(self.fcex2(torch.cat((x_ext1, state_ext), dim=1)))\n",
    "        \n",
    "        x2 = F.relu(self.fc2(torch.cat((x_ego, x_ext2), dim=1)))\n",
    "        return torch.tanh(self.fc3(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae238fd-51ce-4f03-a50c-f7ea14c77301",
   "metadata": {},
   "source": [
    "### Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "103e166c-360e-411c-a3f7-b2e63a25247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_ego_size, state_ext_size, action_size, seed, fc1_units=256, fc2_units=256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fceg1 = nn.Linear(state_ego_size, fc1_units)                                     # egocentric information (14)\n",
    "        \n",
    "        self.fcex1 = nn.Linear(state_ext_size, fc1_units)                                     # exteroceptive information (10)\n",
    "        self.fcex2 = nn.Linear(fc1_units + state_ext_size, fc1_units + state_ext_size)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc1_units + fc1_units + state_ext_size + action_size, fc2_units) # merge [egocentric + exteroceptive](24) + action\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "\n",
    "    def forward(self, state_ego, state_ext, action):\n",
    "        x_ego = F.relu(self.fceg1(state_ego))\n",
    "        \n",
    "        x_ext1 = F.relu(self.fcex1(state_ext))\n",
    "        x_ext2 = F.relu(self.fcex2(torch.cat((x_ext1, state_ext), dim=1)))\n",
    "        \n",
    "        x2 = F.relu(self.fc2(torch.cat((x_ego, x_ext2, action), dim=1)))\n",
    "        return self.fc3(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e547ed-4f0a-4818-aab6-b67539bd64b1",
   "metadata": {},
   "source": [
    "## DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de92d82-9a80-4a3d-b40a-b558897f0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, reward_normalizer, n_critics = 3, random_seed = 0):\n",
    "        self.state_size = state_size\n",
    "        self.state_ego_size = 14\n",
    "        self.state_ext_size = 10\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # actor optimizers\n",
    "        # adam_opti_actor = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        # lion_opti_actor = Lion(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        optimizer = Lion if OPTIM == \"LION\" else optim.Adam\n",
    "        # Actor Network \n",
    "        self.actor_local = Actor(self.state_ego_size, self.state_ext_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(self.state_ego_size, self.state_ext_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optimizer(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        self.actor_loss_history = []\n",
    "        self.actor_loss_history_mean = []\n",
    "\n",
    "        # Critic optimizers\n",
    "        # adam_opti_critics = optim.Adam(self.critic_local[idx].parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        # lion_opti_actor = Lion(self.critic_local[idx].parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.n_critics = n_critics\n",
    "        self.critic_local = [Critic(self.state_ego_size, self.state_ext_size, action_size, random_seed).to(device) for _ in range(self.n_critics)]\n",
    "        self.critic_target = [Critic(self.state_ego_size, self.state_ext_size, action_size, random_seed).to(device) for _ in range(self.n_critics)]\n",
    "        self.critic_optimizer = [optimizer(self.critic_local[idx].parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY) for idx in range(self.n_critics)]\n",
    "        self.critic_loss_history = [[] for _ in range(self.n_critics)]\n",
    "        self.critic_loss_history_mean = [[] for _ in range(self.n_critics)]\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        \n",
    "        # Reward Normalizer\n",
    "        self.reward_normalizer = reward_normalizer\n",
    "        \n",
    "        self.n_updates = 0\n",
    "\n",
    "    def calculate_loss_history(self):\n",
    "        # removing nan\n",
    "        x = np.array([self.actor_loss_history])\n",
    "        x = x[~np.isnan(x)]\n",
    "        # calculate mean\n",
    "        if len(x) > 0:\n",
    "            self.actor_loss_history_mean.append(np.mean(x))\n",
    "        # re initialize history\n",
    "        self.actor_loss_history = []\n",
    "        # free memory\n",
    "        del x\n",
    "\n",
    "        for idx in range(self.n_critics):\n",
    "            # removing nan\n",
    "            x = np.array([self.critic_loss_history[idx]])\n",
    "            x = x[~np.isnan(x)]\n",
    "            # calculate mean\n",
    "            if len(x) > 0:\n",
    "                self.critic_loss_history_mean[idx].append(np.mean(x))\n",
    "            # re initialize history\n",
    "            self.critic_loss_history[idx] = []\n",
    "            # free memory\n",
    "            del x\n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done, timestep):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % POLICY_FREQ == 0:\n",
    "            experiences = self.memory.sample()\n",
    "            states, actions, rewards, next_states, dones = experiences\n",
    "            # switch to numpy\n",
    "            states = states.cpu().numpy()\n",
    "            next_states = next_states.cpu().numpy()\n",
    "            rewards = rewards.cpu().numpy()\n",
    "            # Normalize states\n",
    "            normalizer.observe2d(states)\n",
    "            states = normalizer.normalize(states)\n",
    "            # Normalize next_states\n",
    "            normalizer.observe2d(next_states)\n",
    "            next_states = normalizer.normalize(next_states)\n",
    "            # Normalize rewards\n",
    "            self.reward_normalizer.observe2d(rewards)\n",
    "            rewards = self.reward_normalizer.normalize(rewards)\n",
    "            # switch to tensor\n",
    "            states = torch.from_numpy(states).float().to(device)\n",
    "            next_states = torch.from_numpy(next_states).float().to(device)\n",
    "            rewards = torch.from_numpy(rewards).float().to(device)\n",
    "            self.learn((states, actions, rewards, next_states, dones), GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state[:, :self.state_ego_size], state[:, -self.state_ext_size:]).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            noise = np.random.normal(0, POLICY_NOISE, size=self.action_size)\n",
    "            noise = np.clip(noise, -NOISE_CLIP, NOISE_CLIP)\n",
    "            action += noise\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.actor_local.state_dict(), f\"{PATH}_actor_model.pth\")\n",
    "        for idx, critic_local in enumerate(self.critic_local):\n",
    "            torch.save(critic_local.state_dict(), f\"{PATH}_critic{idx}_model.pth\")\n",
    "\n",
    "    def load_model(self):\n",
    "        self.actor_local.load_state_dict(torch.load(f\"{PATH}_actor_model.pth\"))\n",
    "        for idx in range(self.n_critics):\n",
    "            self.critic_local[idx].load_state_dict(torch.load(f\"{PATH}_critic{idx}_model.pth\"))\n",
    "        \n",
    "    def get_lr(self):\n",
    "        for param_group in self.actor_optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        noise = torch.normal(torch.zeros(actions.size()), POLICY_NOISE).to(device)\n",
    "        noise = torch.clamp(noise, -NOISE_CLIP, NOISE_CLIP)\n",
    "        actions_next = self.actor_target(next_states[:, :self.state_ego_size], next_states[:, -self.state_ext_size:]) + noise\n",
    "        actions_next = torch.clamp(actions_next, -1, 1)\n",
    "\n",
    "        Q_targets_next_arr = [self.critic_target[idx](next_states[:, :self.state_ego_size], next_states[:, -self.state_ext_size:], actions_next) for idx in range(self.n_critics)]\n",
    "\n",
    "        Q_targets_next = Q_targets_next_arr[0]\n",
    "        for idx in range(1, self.n_critics):\n",
    "            Q_targets_next = torch.min(Q_targets_next, Q_targets_next_arr[idx])\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Compute all critic loss\n",
    "        for idx in range(self.n_critics):\n",
    "            # Zero gradients\n",
    "            self.critic_optimizer[idx].zero_grad()\n",
    "            # Compute critic loss\n",
    "            Q_expected = self.critic_local[idx](states[:, :self.state_ego_size], states[:, -self.state_ext_size:], actions)\n",
    "            critic_loss = F.mse_loss(Q_expected, Q_targets.detach()) # detach Q_targets here to avoid the \"the graph are freed\" error\n",
    "            # Save the critic loss\n",
    "            self.critic_loss_history[idx].append(critic_loss.item())\n",
    "            # Backward pass\n",
    "            critic_loss.backward()\n",
    "            # Update weights\n",
    "            self.critic_optimizer[idx].step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Zero gradients\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        # Compute critic loss\n",
    "        actions_pred = self.actor_local(states[:, :self.state_ego_size], states[:, -self.state_ext_size:])\n",
    "        # Compute the average of the critics\n",
    "        critic_values = [self.critic_local[idx](states[:, :self.state_ego_size], states[:, -self.state_ext_size:], actions_pred) for idx in range(self.n_critics)]\n",
    "        critic_value = sum(critic_values) / float(self.n_critics)\n",
    "        actor_loss = -critic_value.mean()\n",
    "        # Save the loss\n",
    "        self.actor_loss_history.append(actor_loss.item())\n",
    "        # Backward pass\n",
    "        actor_loss.backward()\n",
    "        # Update weights\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        for idx in range(self.n_critics):\n",
    "            self.soft_update(self.critic_local[idx], self.critic_target[idx], TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        \n",
    "        self.n_updates += 1\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.actor_local)\n",
    "        print(self.critic_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8bccf-b22e-41c3-ba08-8043bd439e2a",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Classic ddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9e8e6f-725d-4078-b859-5707afafc212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(env, agent, normalizer, reward_normalizer, n_episodes=1000, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    timesteps_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    start_time_elapsed = time.time()\n",
    "    total_timesteps = 0\n",
    "    min_rew = -1500 \n",
    "    ep_min_rew = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # function to override printlines from previous loop iteration \n",
    "        clear_output(wait=True)\n",
    "        state, _ = env.reset()\n",
    "        # State normalization\n",
    "        normalizer.observe(state)\n",
    "        normalized_state = normalizer.normalize(state)\n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            if total_timesteps >= START_LEARNING:\n",
    "                action = agent.act(normalized_state).squeeze(0)\n",
    "            else:\n",
    "                action = env.action_space.sample()  # Sample random action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = torch.from_numpy(next_state.T).float().squeeze(0).numpy()\n",
    "            \n",
    "            # State normalization\n",
    "            normalizer.observe(next_state)\n",
    "            normalized_next_state = normalizer.normalize(next_state)\n",
    "        \n",
    "            done = terminated or truncated\n",
    "            agent.step(state, action, reward, next_state, done, total_timesteps)\n",
    "            state = next_state\n",
    "            normalized_state = normalized_next_state\n",
    "            score += reward\n",
    "            timestep += 1\n",
    "            total_timesteps += 1\n",
    "        scores_deque.append(score)\n",
    "        timesteps_deque.append(timestep)\n",
    "        scores.append(score)\n",
    "        agent.calculate_loss_history()\n",
    "\n",
    "        time_elapsed = time.time() - start_time_elapsed\n",
    "        if min_rew <= score:\n",
    "            min_rew = score\n",
    "            ep_min_rew = i_episode\n",
    "            agent.save_model()\n",
    "            normalizer.save()\n",
    "            reward_normalizer.save()\n",
    "        print_infos(\n",
    "            int(np.mean(timesteps_deque)),\n",
    "            np.mean(scores_deque),\n",
    "            timestep,\n",
    "            score,\n",
    "            min_rew,\n",
    "            ep_min_rew, \n",
    "            i_episode,\n",
    "            int(total_timesteps / time_elapsed),\n",
    "            int(time_elapsed),\n",
    "            total_timesteps,\n",
    "            np.mean(agent.actor_loss_history_mean[-100:]),\n",
    "            [np.mean(critic_loss_history) for critic_loss_history in agent.critic_loss_history_mean[-100:]],\n",
    "            agent.get_lr(),\n",
    "            agent.n_updates\n",
    "        )\n",
    "\n",
    "        if np.mean(scores_deque) >= 300:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            agent.save_model()\n",
    "            normalizer.save()\n",
    "            reward_normalizer.save()\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321dd5b3-6430-4854-8ed1-0566dd76e768",
   "metadata": {},
   "source": [
    "### create environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f46c5-fed4-4da3-8a3f-22afc27a170a",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00717104-4277-4c22-b74a-f47d38c62f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| BUFFER_SIZE=1000000 \t\t |\n",
      "| BATCH_SIZE=128 \t\t |\n",
      "| GAMMA=0.98 \t\t\t |\n",
      "| TAU=0.02 \t\t\t |\n",
      "| LR_ACTOR=5e-05 \t\t |\n",
      "| LR_CRITIC=0.0002 \t\t |\n",
      "| WEIGHT_DECAY=0.0 \t\t |\n",
      "| POLICY_NOISE=0.2 \t\t |\n",
      "| NOISE_CLIP=0.5 \t\t |\n",
      "| POLICY_FREQ=2 \t\t |\n",
      "| START_LEARNING=10000 \t\t |\n",
      "| N_TOTAL_EPISODES=1000 \t |\n",
      "| N_CRITICS=2 \t\t\t |\n",
      "| OPTIM='ADAM' \t\t\t |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb6b094-0a47-4d31-8229-05f6b8a3d700",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4437285-c82d-4e27-b32f-88aae4f640d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc52d1-922e-49e0-87d8-c9331c0981a3",
   "metadata": {},
   "source": [
    "#### Normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "124ae2ce-2616-45b0-842b-8a00c7f10434",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(state_size)\n",
    "reward_normalizer = RewardNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f040fc7-85a7-49f5-a84a-c61f6e52eb7c",
   "metadata": {},
   "source": [
    "#### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce0f9909-f978-4d6c-80e9-3df71dd2e29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (fceg1): Linear(in_features=14, out_features=256, bias=True)\n",
      "  (fcex1): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (fcex2): Linear(in_features=266, out_features=266, bias=True)\n",
      "  (fc2): Linear(in_features=522, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "[Critic(\n",
      "  (fceg1): Linear(in_features=14, out_features=256, bias=True)\n",
      "  (fcex1): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (fcex2): Linear(in_features=266, out_features=266, bias=True)\n",
      "  (fc2): Linear(in_features=526, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      "), Critic(\n",
      "  (fceg1): Linear(in_features=14, out_features=256, bias=True)\n",
      "  (fcex1): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (fcex2): Linear(in_features=266, out_features=266, bias=True)\n",
      "  (fc2): Linear(in_features=526, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size, action_size, reward_normalizer, N_CRITICS, random_seed=0)\n",
    "agent.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccf1478-e249-422e-918a-2794c7214bbb",
   "metadata": {},
   "source": [
    "#### Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4218e8cb-dec0-4b9c-9d18-48a1cf2f4416",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_normalizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_TOTAL_EPISODES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[19], line 32\u001b[0m, in \u001b[0;36mddpg\u001b[1;34m(env, agent, normalizer, reward_normalizer, n_episodes, max_t)\u001b[0m\n\u001b[0;32m     29\u001b[0m normalized_next_state \u001b[38;5;241m=\u001b[39m normalizer\u001b[38;5;241m.\u001b[39mnormalize(next_state)\n\u001b[0;32m     31\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m---> 32\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     34\u001b[0m normalized_state \u001b[38;5;241m=\u001b[39m normalized_next_state\n",
      "Cell \u001b[1;32mIn[18], line 86\u001b[0m, in \u001b[0;36mAgent.step\u001b[1;34m(self, state, action, reward, next_state, done, timestep)\u001b[0m\n\u001b[0;32m     84\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(next_states)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     85\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(rewards)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGAMMA\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 143\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m    141\u001b[0m     critic_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_optimizer\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# ---------------------------- update actor ---------------------------- #\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Zero gradients\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\.virtualenvs\\box2d-pmArwpNk\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\box2d-pmArwpNk\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\.virtualenvs\\box2d-pmArwpNk\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\.virtualenvs\\box2d-pmArwpNk\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = ddpg(env, agent, normalizer, reward_normalizer, N_TOTAL_EPISODES)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1066bda-dc92-477a-9944-b58443acf4de",
   "metadata": {},
   "source": [
    "### Plot rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe5527-b2a7-4b24-a25c-ccee897a002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the point history\n",
    "plot_history(scores, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0de69-f5a8-445e-bb47-5d0b5ec8853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the point Actor loss history\n",
    "plot_history(agent.actor_loss_history_mean, 100, y_label=\"Actor Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce5b25-ff52-4a50-bc13-8c7956494963",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(agent.n_critics):\n",
    "    plot_history(agent.critic_loss_history_mean[idx], 100, y_label=f\"Critic Loss {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5096fc4c-0359-4861-a411-0ef6773aaf33",
   "metadata": {},
   "source": [
    "## Visualizing\n",
    "### create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4260759-3660-440c-90ee-34d027a9d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"rgb_array\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "new_normalizer = Normalizer(state_size)\n",
    "new_normalizer.load()  # Load the saved statistics\n",
    "new_reward_normalizer = RewardNormalizer()\n",
    "new_reward_normalizer.load()\n",
    "agent2 = Agent(state_size, action_size, new_reward_normalizer, random_seed=0)\n",
    "agent2.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4679d-c3a6-462a-9f49-81205757ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_list = []\n",
    "scores = []\n",
    "best_score = -1500\n",
    "for i in range(10):\n",
    "    state, _ = env.reset()\n",
    "    #Select an action\n",
    "    normalized_state = new_normalizer.normalize(state)\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    episode_reward = 0\n",
    "    e_screen_list = []\n",
    "    while not done:\n",
    "        action = agent2.act(normalized_state, False).squeeze(0)\n",
    "    \n",
    "        # Printing env render (rgb_array)\n",
    "        screen = env.render()\n",
    "        # Add title to the screen\n",
    "        screen = cv2.putText(\n",
    "            np.array(screen),\n",
    "            f\"Iteration=[{i}] Timestep=[{timestep +1}]\",\n",
    "            (25, 25),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0, 0, 0),\n",
    "            1,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "        e_screen_list.append(screen)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = torch.from_numpy(next_state.T).float().squeeze(0).numpy()\n",
    "        done = terminated or truncated\n",
    "    \n",
    "        #Select an action\n",
    "        normalized_next_state = new_normalizer.normalize(next_state)\n",
    "        state = next_state\n",
    "        normalized_state = normalized_next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "        timestep += 1\n",
    "\n",
    "    if best_score <= episode_reward:\n",
    "        best_score = episode_reward\n",
    "        screen_list.append(e_screen_list)\n",
    "    \n",
    "    scores.append(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2bc83-9209-4f0b-b5fb-add05b51f782",
   "metadata": {},
   "source": [
    "### Scores of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5f20a-3240-44d4-bc69-9dc9d92679c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best score: {max(scores)}\")\n",
    "print(f\"AVG score: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b048482-0467-4d37-a2b2-4373536faf83",
   "metadata": {},
   "source": [
    "### Selection of the 5 best iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d31bb-0059-4937-a560-2297998f8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7027292-deb8-4d5b-97f0-b6d13b2ee90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_screen_list = screen_list[-n:]\n",
    "print(f\"Number of iter : {len(best_screen_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880138d-5a21-48df-8515-b08910659931",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_screens_list = np.empty((0, 400, 600, 3))\n",
    "for screen_ep in best_screen_list:\n",
    "    selected_screens_list = np.concatenate((selected_screens_list, np.array(screen_ep)), axis=0)\n",
    "\n",
    "selected_screens_list = selected_screens_list.astype(np.uint8)\n",
    "print(selected_screens_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c78e8-8329-4a24-9cbe-8efecce85feb",
   "metadata": {},
   "source": [
    "### Save gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5e5c7-2842-48e8-ae1b-9427fb38a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{PATH_IMG}.gif\"\n",
    "save_gif(list(selected_screens_list), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b76ee-24d9-4a92-90c1-90d2ca259625",
   "metadata": {},
   "source": [
    "### Embed the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f68a2-d370-4a65-87f2-21a06d9c81f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = open(path, 'rb').read()\n",
    "b64_video = base64.b64encode(video)\n",
    "video_tag = '<img src=\"data:image/gif;base64,{0}\">'.format(b64_video.decode())\n",
    "\n",
    "display(HTML(video_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34afa4f8-65cc-4e86-b670-0a5d46a56cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the point scores evaluation\n",
    "plot_history(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d696b-6aa9-4827-b0d5-af5119d8500a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe16936-08b3-4bff-ac8f-fa581d4a2dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d704a-e7f6-4df2-96a2-7d9e6f36a80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
