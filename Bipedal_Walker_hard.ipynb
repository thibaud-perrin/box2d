{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fb422d-6fb0-4159-b6e5-79bff41dc79b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bipedal Walker (hardcore)\n",
    "[Documentation](https://gymnasium.farama.org/environments/box2d/bipedal_walker/)  \n",
    "This environment is part of the Box2D environments which contains general information about the environment.\n",
    "\n",
    "[leaderboard](https://github.com/openai/gym/wiki/Leaderboard#bipedalwalker-v2)\n",
    "\n",
    "## Description\n",
    "This is a simple 4-joint walker robot environment. There are two versions:\n",
    "\n",
    "- Normal, with slightly uneven terrain.\n",
    "- Hardcore, with ladders, stumps, pitfalls.\n",
    "ls.\r\n",
    "\r\n",
    "To solve the normal version, you need to get 300 points in 1600 time steps. To solve the hardcore version, you need 300 points in 2000 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952bb491-c1c2-4d92-b0da-156f9025fa30",
   "metadata": {},
   "source": [
    "We have decided to used the DDPG algorithme with the TD3 variant, we used 5 critics and D2RL architecture model to help them to learn. We also use normalization on states and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28409768-9bc1-44c3-82ed-0ca9e0b7f251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import os\n",
    "import base64\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# import third-party libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "import imageio\n",
    "import cv2\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc43622-7836-473b-914d-6c143a7c5cb7",
   "metadata": {},
   "source": [
    "## Hardware infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fd67c7-c94a-49c2-90f7-e3ed1ea56ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1070 (UUID: GPU-19cef1c2-e216-e824-c98e-660394f8a4bb)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a786ed-9ed2-4f0b-ad4e-f1f22b4c0b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version.cuda=[11.7]\n",
      "torch.cuda.is_available(True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch.version.cuda=[{torch.version.cuda}]\")\n",
    "print(f\"torch.cuda.is_available({torch.cuda.is_available()})\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62e894b-412c-420a-bf10-7ede8eff4e55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232c832-cd45-432f-a602-4442a0105b43",
   "metadata": {},
   "source": [
    "## Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dc8a76-424c-4823-afb3-b04e7a116279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./img/bipedal_walker_hardcore :\n",
      "total 14004\n",
      "drwxr-xr-x 1 thiba 197609        0 Jun 26 09:34 .\n",
      "drwxr-xr-x 1 thiba 197609        0 Jun 26 09:31 ..\n",
      "-rw-r--r-- 1 thiba 197609 14334369 Jun 26 21:39 bipedal_walker_hardcore.gif\n",
      "./saves/bipedal_walker_hardcore :\n",
      "total 5033\n",
      "drwxr-xr-x 1 thiba 197609      0 Jun 26 16:49 .\n",
      "drwxr-xr-x 1 thiba 197609      0 Jun 26 09:31 ..\n",
      "drwxr-xr-x 1 thiba 197609      0 Jun 26 16:49 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 thiba 197609 854167 Jun 26 22:39 bipedal_walker_hardcore_actor_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 855215 Jun 26 22:39 bipedal_walker_hardcore_critic0_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 855215 Jun 26 22:39 bipedal_walker_hardcore_critic1_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 855215 Jun 26 22:39 bipedal_walker_hardcore_critic2_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 855215 Jun 26 22:39 bipedal_walker_hardcore_critic3_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 855215 Jun 26 22:39 bipedal_walker_hardcore_critic4_model.pth\n",
      "-rw-r--r-- 1 thiba 197609   1031 Jun 26 22:39 bipedal_walker_hardcore_normalizer.pkl\n",
      "-rw-r--r-- 1 thiba 197609    237 Jun 26 22:39 bipedal_walker_hardcore_reward_normalizer.pkl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('img/bipedal_walker_hardcore', exist_ok=True)\n",
    "os.makedirs('saves/bipedal_walker_hardcore', exist_ok=True)\n",
    "print('./img/bipedal_walker_hardcore :')\n",
    "!ls -al \"img/bipedal_walker_hardcore\"\n",
    "print('./saves/bipedal_walker_hardcore :')\n",
    "!ls -al \"saves/bipedal_walker_hardcore\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc8f7e-4ed4-4a9e-b222-f36d75a8efdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constants\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c447b469-f41b-48a4-98fd-bea9449be473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1_000_000\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.98\n",
    "TAU = 0.02\n",
    "LR_ACTOR = 0.0001\n",
    "LR_CRITIC = 0.0003\n",
    "WEIGHT_DECAY = 0.00\n",
    "POLICY_NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "POLICY_FREQ = 2\n",
    "START_LEARNING = 10000 # steps in the beginning to let the agent explore\n",
    "N_TOTAL_EPISODES = 3000\n",
    "N_CRITICS = 5\n",
    "OPTIM = \"ADAM\" # \"LION\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fece19-c43a-45aa-95b1-b8a1a42ede8c",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c6b75d-0fe2-4209-ba7c-ceb424985e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./saves/bipedal_walker_hardcore/bipedal_walker_hardcore\"\n",
    "PATH_IMG = \"./img/bipedal_walker_hardcore/bipedal_walker_hardcore\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2e836-6d3c-411e-a629-f71c7ee5df96",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a31f47-dea9-4943-affa-2c3463dd42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_infos(ep_len_mean, ep_rew_mean, ep_len, ep_rew, min_rew, ep_min_rew, episodes, fps, time_elapsed, total_timesteps, actor_loss, critic_loss_arr, learning_rate, n_updates):\n",
    "    print(f\"- rollout/\")\n",
    "    print(f\"    - ep_len_mean     : {ep_len_mean}\")\n",
    "    print(f\"    - ep_rew_mean     : {ep_rew_mean}\")\n",
    "    print(f\"    - ep_len          : {ep_len}\")\n",
    "    print(f\"    - ep_rew          : {ep_rew}\")\n",
    "    print(f\"    - max_rew         : {min_rew}\")\n",
    "    print(f\"    - ind_max_rew     : {ep_min_rew}\")\n",
    "    print(f\"\")\n",
    "    print(f\"- time/\")\n",
    "    print(f\"    - episodes        : {episodes}\")\n",
    "    print(f\"    - fps             : {fps}\")\n",
    "    print(f\"    - time_elapsed    : {time_elapsed}\")\n",
    "    print(f\"    - total_timesteps : {total_timesteps}\")\n",
    "    print(f\"\")\n",
    "    print(f\"- train/\")\n",
    "    print(f\"    - actor_loss      : {actor_loss}\")\n",
    "    for i, c in enumerate(critic_loss_arr):\n",
    "        print(f\"    - critic_loss{i+1}    : {c}\")\n",
    "    print(f\"    - learning_rate   : {learning_rate}\")\n",
    "    print(f\"    - n_updates       : {n_updates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f0e68e6-212f-4df7-b36b-fe425b1a0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gif(img_list, path):\n",
    "    # Convert the list of frames to a numpy array\n",
    "    resized_img_array = []\n",
    "    for img in img_list:\n",
    "        img_pil = Image.fromarray(img)\n",
    "        # Make sure width and height are divisible by 16\n",
    "        img_resized_pil = img_pil.resize((608, 400))\n",
    "        img_resized = np.array(img_resized_pil)\n",
    "        resized_img_array.append(img_resized)\n",
    "    \n",
    "    # Create gif video\n",
    "    fps = 20\n",
    "    imageio.mimsave(path, resized_img_array, 'GIF', duration=int(1000 * 1/fps), loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b22232-a375-4c23-ba54-c7a818c19eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(reward_history, rolling_window=20, x_label = 'Episode', y_label = 'Total Points', lower_limit=None, upper_limit=None, plot_rw=True, plot_rm=True):\n",
    "    \"\"\"\n",
    "    Function to plot reward history and its rolling mean with some optional arguments.\n",
    "\n",
    "    Args:\n",
    "        reward_history (list): A list of rewards for each episode.\n",
    "        rolling_window (int): The number of episodes for computing the rolling mean.\n",
    "        lower_limit (int): Starting episode index for plotting.\n",
    "        upper_limit (int): Ending episode index for plotting.\n",
    "        plot_rw (bool): A flag for plotting raw reward history.\n",
    "        plot_rm (bool): A flag for plotting rolling mean reward history.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # If lower_limit and upper_limit are not provided, use the whole reward_history\n",
    "    if lower_limit is None or upper_limit is None:\n",
    "        rh = reward_history\n",
    "        xs = [x for x in range(len(reward_history))]\n",
    "    else:\n",
    "        rh = reward_history[lower_limit:upper_limit]\n",
    "        xs = [x for x in range(lower_limit,upper_limit)]\n",
    "   \n",
    "    # Create a DataFrame and calculate the rolling mean\n",
    "    df = pd.DataFrame(rh)\n",
    "    rollingMean = df.rolling(rolling_window).mean()\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10,7), facecolor='white')\n",
    "    \n",
    "    if plot_rw:\n",
    "        plt.plot(xs, rh, linewidth=1, color='cyan')\n",
    "    if plot_rm:\n",
    "        plt.plot(xs, rollingMean, linewidth=2, color='magenta')\n",
    "\n",
    "    text_color = 'black'\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('black')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.xlabel(x_label, color=text_color, fontsize=30)\n",
    "    plt.ylabel(y_label, color=text_color, fontsize=30)\n",
    "    yNumFmt = mticker.StrMethodFormatter('{x:,}')\n",
    "    ax.yaxis.set_major_formatter(yNumFmt)\n",
    "    ax.tick_params(axis='x', colors=text_color)\n",
    "    ax.tick_params(axis='y', colors=text_color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa23f826-b9fd-4a24-bc04-f587bd7c4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hyperparameters():\n",
    "    print(\"----------------------------------\")\n",
    "    print(f\"| {BUFFER_SIZE=} \\t\\t |\")\n",
    "    print(f\"| {BATCH_SIZE=} \\t\\t |\")\n",
    "    print(f\"| {GAMMA=} \\t\\t\\t |\")\n",
    "    print(f\"| {TAU=} \\t\\t\\t |\")\n",
    "    print(f\"| {LR_ACTOR=} \\t\\t |\")\n",
    "    print(f\"| {LR_CRITIC=} \\t\\t |\")\n",
    "    print(f\"| {WEIGHT_DECAY=} \\t\\t |\")\n",
    "    print(f\"| {POLICY_NOISE=} \\t\\t |\")\n",
    "    print(f\"| {NOISE_CLIP=} \\t\\t |\")\n",
    "    print(f\"| {POLICY_FREQ=} \\t\\t |\")\n",
    "    print(f\"| {START_LEARNING=} \\t\\t |\")\n",
    "    print(f\"| {N_TOTAL_EPISODES=} \\t |\")\n",
    "    print(f\"| {N_CRITICS=} \\t\\t\\t |\")\n",
    "    print(f\"| {OPTIM=} \\t\\t\\t |\")\n",
    "    print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d16eb8-5c5f-4aed-9bdb-b3e6724da8c9",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb680d6e-63ed-46d6-a891-cf2588682ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a453c0e-37c9-4aea-b294-30527387c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838f137-bd48-4670-b25c-cc0e90330274",
   "metadata": {},
   "source": [
    "## Normalizer\n",
    "### States normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28650759-d2b7-413d-81e7-0ae8d5c698ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    def __init__(self, size):\n",
    "        self.n = np.zeros(size)\n",
    "        self.mean = np.zeros(size)\n",
    "        self.M2 = np.zeros(size)\n",
    "        self.var = np.full(size, 1e-2)\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1.0\n",
    "        delta = x - self.mean\n",
    "        self.mean += delta / self.n\n",
    "        delta2 = x - self.mean\n",
    "        self.M2 += delta * delta2\n",
    "        less_than_two = self.n < 2\n",
    "        self.var = np.where(less_than_two, np.full(self.var.shape, 1e-2), self.M2 / (self.n - 1))\n",
    "        self.var = np.maximum(self.var, 1e-2)  # Clip the variance to a minimum value\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - self.mean) / obs_std\n",
    "\n",
    "    def observe2d(self, x):\n",
    "        x = np.atleast_2d(x)  # Make sure x is at least 2-D\n",
    "        batch_size, _ = x.shape\n",
    "        self.n += batch_size\n",
    "        for instance in x:\n",
    "            delta = instance - self.mean\n",
    "            self.mean += delta / self.n\n",
    "            delta2 = instance - self.mean\n",
    "            self.M2 += delta * delta2\n",
    "        less_than_two = self.n < 2\n",
    "        self.var = np.where(less_than_two, np.full(self.var.shape, 1e-2), self.M2 / (self.n - 1))\n",
    "        self.var = np.maximum(self.var, 1e-2)  # Clip the variance to a minimum value\n",
    "\n",
    "    def save(self):\n",
    "        with open(f\"{PATH}_normalizer.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "\n",
    "    def load(self):\n",
    "        with open(f\"{PATH}_normalizer.pkl\", 'rb') as f:\n",
    "            tmp_dict = pickle.load(f)\n",
    "        self.__dict__.update(tmp_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdc7cc-2c7e-4be6-ad87-91425329ae60",
   "metadata": {},
   "source": [
    "### Reward normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c959ac55-5f34-4798-a9be-8bea38bcf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNormalizer():\n",
    "    def __init__(self, dim=1):\n",
    "        self.n = 0\n",
    "        self.dim = dim\n",
    "        self.mean = np.zeros(self.dim)\n",
    "        self.M2 = np.zeros(self.dim)\n",
    "        self.var = np.full(self.dim, 1e-2)\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1\n",
    "        delta = x - self.mean\n",
    "        self.mean += delta / self.n\n",
    "        delta2 = x - self.mean\n",
    "        self.M2 += delta * delta2\n",
    "        if self.n < 2:\n",
    "            self.var = np.full(self.dim, 1e-2)\n",
    "        else:\n",
    "            self.var = self.M2 / (self.n - 1)\n",
    "        self.var = np.maximum(self.var, 1e-2)  # Clip the variance to a minimum value\n",
    "\n",
    "    def normalize(self, reward):\n",
    "        reward_std = np.sqrt(self.var)\n",
    "        return (reward - self.mean) / reward_std\n",
    "\n",
    "    def observe2d(self, x):\n",
    "        x = np.atleast_2d(x)  # Make sure x is at least 2-D\n",
    "        batch_size, dim = x.shape\n",
    "        assert dim == self.dim, \"Input dimensions do not match initialized dimensions\"\n",
    "        self.n += batch_size\n",
    "        for instance in x:\n",
    "            delta = instance - self.mean\n",
    "            self.mean += delta / self.n\n",
    "            delta2 = instance - self.mean\n",
    "            self.M2 += delta * delta2\n",
    "        if self.n < 2:\n",
    "            self.var = np.full(self.dim, 1e-2)\n",
    "        else:\n",
    "            self.var = self.M2 / (self.n - 1)\n",
    "        self.var = np.maximum(self.var, 1e-2)  # Clip the variance to a minimum value\n",
    "\n",
    "    def save(self):\n",
    "        with open(f\"{PATH}_reward_normalizer.pkl\", 'wb') as f:\n",
    "            pickle.dump((self.n, self.mean, self.M2, self.var), f)\n",
    "\n",
    "    def load(self):\n",
    "        with open(f\"{PATH}_reward_normalizer.pkl\", 'rb') as f:\n",
    "            self.n, self.mean, self.M2, self.var = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d0fb2-6a15-47d0-b950-cda0633cd3d5",
   "metadata": {},
   "source": [
    "## Network\n",
    "### Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9310d32-699c-45a5-a79c-39dde680481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_ego_size, state_ext_size, action_size, seed, fc1_units=256, fc2_units=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fceg1 = nn.Linear(state_ego_size, fc1_units)                                     # egocentric information (14)\n",
    "        \n",
    "        self.fcex1 = nn.Linear(state_ext_size, fc1_units)                                     # exteroceptive information (10)\n",
    "        self.fcex2 = nn.Linear(fc1_units + state_ext_size, fc1_units + state_ext_size)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc1_units + fc1_units + state_ext_size, fc2_units)               # merge [egocentric + exteroceptive](24)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state_ego, state_ext):\n",
    "        x_ego = F.relu(self.fceg1(state_ego))\n",
    "        \n",
    "        x_ext1 = F.relu(self.fcex1(state_ext))\n",
    "        x_ext2 = F.relu(self.fcex2(torch.cat((x_ext1, state_ext), dim=1)))\n",
    "        \n",
    "        x2 = F.relu(self.fc2(torch.cat((x_ego, x_ext2), dim=1)))\n",
    "        return torch.tanh(self.fc3(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae238fd-51ce-4f03-a50c-f7ea14c77301",
   "metadata": {},
   "source": [
    "### Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "103e166c-360e-411c-a3f7-b2e63a25247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_ego_size, state_ext_size, action_size, seed, fc1_units=256, fc2_units=256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fceg1 = nn.Linear(state_ego_size, fc1_units)                                     # egocentric information (14)\n",
    "        \n",
    "        self.fcex1 = nn.Linear(state_ext_size, fc1_units)                                     # exteroceptive information (10)\n",
    "        self.fcex2 = nn.Linear(fc1_units + state_ext_size, fc1_units + state_ext_size)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc1_units + fc1_units + state_ext_size + action_size, fc2_units) # merge [egocentric + exteroceptive](24) + action\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "\n",
    "    def forward(self, state_ego, state_ext, action):\n",
    "        x_ego = F.relu(self.fceg1(state_ego))\n",
    "        \n",
    "        x_ext1 = F.relu(self.fcex1(state_ext))\n",
    "        x_ext2 = F.relu(self.fcex2(torch.cat((x_ext1, state_ext), dim=1)))\n",
    "        \n",
    "        x2 = F.relu(self.fc2(torch.cat((x_ego, x_ext2, action), dim=1)))\n",
    "        return self.fc3(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e547ed-4f0a-4818-aab6-b67539bd64b1",
   "metadata": {},
   "source": [
    "## DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de92d82-9a80-4a3d-b40a-b558897f0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, reward_normalizer, n_critics = 3, random_seed = 0):\n",
    "        self.state_size = state_size\n",
    "        self.state_ego_size = 14\n",
    "        self.state_ext_size = 10\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # actor optimizers\n",
    "        # adam_opti_actor = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        # lion_opti_actor = Lion(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        optimizer = Lion if OPTIM == \"LION\" else optim.Adam\n",
    "        # Actor Network \n",
    "        self.actor_local = Actor(self.state_ego_size, self.state_ext_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(self.state_ego_size, self.state_ext_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optimizer(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        self.actor_loss_history = []\n",
    "        self.actor_loss_history_mean = []\n",
    "\n",
    "        # Critic optimizers\n",
    "        # adam_opti_critics = optim.Adam(self.critic_local[idx].parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        # lion_opti_actor = Lion(self.critic_local[idx].parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.n_critics = n_critics\n",
    "        self.critic_local = [Critic(self.state_ego_size, self.state_ext_size, action_size, random_seed).to(device) for _ in range(self.n_critics)]\n",
    "        self.critic_target = [Critic(self.state_ego_size, self.state_ext_size, action_size, random_seed).to(device) for _ in range(self.n_critics)]\n",
    "        self.critic_optimizer = [optimizer(self.critic_local[idx].parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY) for idx in range(self.n_critics)]\n",
    "        self.critic_loss_history = [[] for _ in range(self.n_critics)]\n",
    "        self.critic_loss_history_mean = [[] for _ in range(self.n_critics)]\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        \n",
    "        # Reward Normalizer\n",
    "        self.reward_normalizer = reward_normalizer\n",
    "        \n",
    "        self.n_updates = 0\n",
    "\n",
    "    def calculate_loss_history(self):\n",
    "        # removing nan\n",
    "        x = np.array([self.actor_loss_history])\n",
    "        x = x[~np.isnan(x)]\n",
    "        # calculate mean\n",
    "        if len(x) > 0:\n",
    "            self.actor_loss_history_mean.append(np.mean(x))\n",
    "        # re initialize history\n",
    "        self.actor_loss_history = []\n",
    "        # free memory\n",
    "        del x\n",
    "\n",
    "        for idx in range(self.n_critics):\n",
    "            # removing nan\n",
    "            x = np.array([self.critic_loss_history[idx]])\n",
    "            x = x[~np.isnan(x)]\n",
    "            # calculate mean\n",
    "            if len(x) > 0:\n",
    "                self.critic_loss_history_mean[idx].append(np.mean(x))\n",
    "            # re initialize history\n",
    "            self.critic_loss_history[idx] = []\n",
    "            # free memory\n",
    "            del x\n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done, timestep):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % POLICY_FREQ == 0:\n",
    "            experiences = self.memory.sample()\n",
    "            states, actions, rewards, next_states, dones = experiences\n",
    "            # switch to numpy\n",
    "            states = states.cpu().numpy()\n",
    "            next_states = next_states.cpu().numpy()\n",
    "            rewards = rewards.cpu().numpy()\n",
    "            # Normalize states\n",
    "            normalizer.observe2d(states)\n",
    "            states = normalizer.normalize(states)\n",
    "            # Normalize next_states\n",
    "            normalizer.observe2d(next_states)\n",
    "            next_states = normalizer.normalize(next_states)\n",
    "            # Normalize rewards\n",
    "            self.reward_normalizer.observe2d(rewards)\n",
    "            rewards = self.reward_normalizer.normalize(rewards)\n",
    "            # switch to tensor\n",
    "            states = torch.from_numpy(states).float().to(device)\n",
    "            next_states = torch.from_numpy(next_states).float().to(device)\n",
    "            rewards = torch.from_numpy(rewards).float().to(device)\n",
    "            self.learn((states, actions, rewards, next_states, dones), GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state[:, :self.state_ego_size], state[:, -self.state_ext_size:]).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            noise = np.random.normal(0, POLICY_NOISE, size=self.action_size)\n",
    "            noise = np.clip(noise, -NOISE_CLIP, NOISE_CLIP)\n",
    "            action += noise\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.actor_local.state_dict(), f\"{PATH}_actor_model.pth\")\n",
    "        for idx, critic_local in enumerate(self.critic_local):\n",
    "            torch.save(critic_local.state_dict(), f\"{PATH}_critic{idx}_model.pth\")\n",
    "\n",
    "    def load_model(self):\n",
    "        self.actor_local.load_state_dict(torch.load(f\"{PATH}_actor_model.pth\"))\n",
    "        for idx in range(self.n_critics):\n",
    "            self.critic_local[idx].load_state_dict(torch.load(f\"{PATH}_critic{idx}_model.pth\"))\n",
    "        \n",
    "    def get_lr(self):\n",
    "        for param_group in self.actor_optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        noise = torch.normal(torch.zeros(actions.size()), POLICY_NOISE).to(device)\n",
    "        noise = torch.clamp(noise, -NOISE_CLIP, NOISE_CLIP)\n",
    "        actions_next = self.actor_target(next_states[:, :self.state_ego_size], next_states[:, -self.state_ext_size:]) + noise\n",
    "        actions_next = torch.clamp(actions_next, -1, 1)\n",
    "\n",
    "        Q_targets_next_arr = [self.critic_target[idx](next_states[:, :self.state_ego_size], next_states[:, -self.state_ext_size:], actions_next) for idx in range(self.n_critics)]\n",
    "\n",
    "        Q_targets_next = Q_targets_next_arr[0]\n",
    "        for idx in range(1, self.n_critics):\n",
    "            Q_targets_next = torch.min(Q_targets_next, Q_targets_next_arr[idx])\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Compute all critic loss\n",
    "        for idx in range(self.n_critics):\n",
    "            # Zero gradients\n",
    "            self.critic_optimizer[idx].zero_grad()\n",
    "            # Compute critic loss\n",
    "            Q_expected = self.critic_local[idx](states[:, :self.state_ego_size], states[:, -self.state_ext_size:], actions)\n",
    "            critic_loss = F.mse_loss(Q_expected, Q_targets.detach()) # detach Q_targets here to avoid the \"the graph are freed\" error\n",
    "            # Save the critic loss\n",
    "            self.critic_loss_history[idx].append(critic_loss.item())\n",
    "            # Backward pass\n",
    "            critic_loss.backward()\n",
    "            # Update weights\n",
    "            self.critic_optimizer[idx].step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Zero gradients\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        # Compute critic loss\n",
    "        actions_pred = self.actor_local(states[:, :self.state_ego_size], states[:, -self.state_ext_size:])\n",
    "        # Compute the average of the critics\n",
    "        critic_values = [self.critic_local[idx](states[:, :self.state_ego_size], states[:, -self.state_ext_size:], actions_pred) for idx in range(self.n_critics)]\n",
    "        critic_value = sum(critic_values) / float(self.n_critics)\n",
    "        actor_loss = -critic_value.mean()\n",
    "        # Save the loss\n",
    "        self.actor_loss_history.append(actor_loss.item())\n",
    "        # Backward pass\n",
    "        actor_loss.backward()\n",
    "        # Update weights\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        for idx in range(self.n_critics):\n",
    "            self.soft_update(self.critic_local[idx], self.critic_target[idx], TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        \n",
    "        self.n_updates += 1\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.actor_local)\n",
    "        print(self.critic_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8bccf-b22e-41c3-ba08-8043bd439e2a",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Classic ddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9e8e6f-725d-4078-b859-5707afafc212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(env, agent, normalizer, reward_normalizer, n_episodes=1000, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    timesteps_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    start_time_elapsed = time.time()\n",
    "    total_timesteps = 0\n",
    "    min_rew = -1500 \n",
    "    ep_min_rew = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # function to override printlines from previous loop iteration \n",
    "        clear_output(wait=True)\n",
    "        state, _ = env.reset()\n",
    "        # State normalization\n",
    "        normalizer.observe(state)\n",
    "        normalized_state = normalizer.normalize(state)\n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            if total_timesteps >= START_LEARNING:\n",
    "                action = agent.act(normalized_state).squeeze(0)\n",
    "            else:\n",
    "                action = env.action_space.sample()  # Sample random action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = torch.from_numpy(next_state.T).float().squeeze(0).numpy()\n",
    "            \n",
    "            # State normalization\n",
    "            normalizer.observe(next_state)\n",
    "            normalized_next_state = normalizer.normalize(next_state)\n",
    "        \n",
    "            done = terminated or truncated\n",
    "            agent.step(state, action, reward, next_state, done, total_timesteps)\n",
    "            state = next_state\n",
    "            normalized_state = normalized_next_state\n",
    "            score += reward\n",
    "            timestep += 1\n",
    "            total_timesteps += 1\n",
    "        scores_deque.append(score)\n",
    "        timesteps_deque.append(timestep)\n",
    "        scores.append(score)\n",
    "        agent.calculate_loss_history()\n",
    "\n",
    "        time_elapsed = time.time() - start_time_elapsed\n",
    "        if min_rew <= score:\n",
    "            min_rew = score\n",
    "            ep_min_rew = i_episode\n",
    "            agent.save_model()\n",
    "            normalizer.save()\n",
    "            reward_normalizer.save()\n",
    "        print_infos(\n",
    "            int(np.mean(timesteps_deque)),\n",
    "            np.mean(scores_deque),\n",
    "            timestep,\n",
    "            score,\n",
    "            min_rew,\n",
    "            ep_min_rew, \n",
    "            i_episode,\n",
    "            int(total_timesteps / time_elapsed),\n",
    "            int(time_elapsed),\n",
    "            total_timesteps,\n",
    "            np.mean(agent.actor_loss_history_mean[-100:]),\n",
    "            [np.mean(critic_loss_history) for critic_loss_history in agent.critic_loss_history_mean[-100:]],\n",
    "            agent.get_lr(),\n",
    "            agent.n_updates\n",
    "        )\n",
    "\n",
    "        if np.mean(scores_deque) >= 300:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            agent.save_model()\n",
    "            normalizer.save()\n",
    "            reward_normalizer.save()\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321dd5b3-6430-4854-8ed1-0566dd76e768",
   "metadata": {},
   "source": [
    "### create environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f46c5-fed4-4da3-8a3f-22afc27a170a",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00717104-4277-4c22-b74a-f47d38c62f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| BUFFER_SIZE=1000000 \t\t |\n",
      "| BATCH_SIZE=256 \t\t |\n",
      "| GAMMA=0.98 \t\t\t |\n",
      "| TAU=0.02 \t\t\t |\n",
      "| LR_ACTOR=0.0001 \t\t |\n",
      "| LR_CRITIC=0.0003 \t\t |\n",
      "| WEIGHT_DECAY=0.0 \t\t |\n",
      "| POLICY_NOISE=0.2 \t\t |\n",
      "| NOISE_CLIP=0.5 \t\t |\n",
      "| POLICY_FREQ=2 \t\t |\n",
      "| START_LEARNING=10000 \t\t |\n",
      "| N_TOTAL_EPISODES=3000 \t |\n",
      "| N_CRITICS=5 \t\t\t |\n",
      "| OPTIM='ADAM' \t\t\t |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb6b094-0a47-4d31-8229-05f6b8a3d700",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4437285-c82d-4e27-b32f-88aae4f640d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc52d1-922e-49e0-87d8-c9331c0981a3",
   "metadata": {},
   "source": [
    "#### Normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "124ae2ce-2616-45b0-842b-8a00c7f10434",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(state_size)\n",
    "reward_normalizer = RewardNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f040fc7-85a7-49f5-a84a-c61f6e52eb7c",
   "metadata": {},
   "source": [
    "#### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce0f9909-f978-4d6c-80e9-3df71dd2e29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (fceg1): Linear(in_features=14, out_features=256, bias=True)\n",
      "  (fcex1): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (fcex2): Linear(in_features=266, out_features=266, bias=True)\n",
      "  (fc2): Linear(in_features=522, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "[Critic(\n",
      "  (fceg1): Linear(in_features=14, out_features=256, bias=True)\n",
      "  (fcex1): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (fcex2): Linear(in_features=266, out_features=266, bias=True)\n",
      "  (fc2): Linear(in_features=526, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      "), Critic(\n",
      "  (fceg1): Linear(in_features=14, out_features=256, bias=True)\n",
      "  (fcex1): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (fcex2): Linear(in_features=266, out_features=266, bias=True)\n",
      "  (fc2): Linear(in_features=526, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      "), Critic(\n",
      "  (fceg1): Linear(in_features=14, out_features=256, bias=True)\n",
      "  (fcex1): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (fcex2): Linear(in_features=266, out_features=266, bias=True)\n",
      "  (fc2): Linear(in_features=526, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      "), Critic(\n",
      "  (fceg1): Linear(in_features=14, out_features=256, bias=True)\n",
      "  (fcex1): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (fcex2): Linear(in_features=266, out_features=266, bias=True)\n",
      "  (fc2): Linear(in_features=526, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      "), Critic(\n",
      "  (fceg1): Linear(in_features=14, out_features=256, bias=True)\n",
      "  (fcex1): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (fcex2): Linear(in_features=266, out_features=266, bias=True)\n",
      "  (fc2): Linear(in_features=526, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size, action_size, reward_normalizer, N_CRITICS, random_seed=0)\n",
    "agent.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccf1478-e249-422e-918a-2794c7214bbb",
   "metadata": {},
   "source": [
    "#### Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218e8cb-dec0-4b9c-9d18-48a1cf2f4416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- rollout/\n",
      "    - ep_len_mean     : 260\n",
      "    - ep_rew_mean     : -117.91870902262107\n",
      "    - ep_len          : 53\n",
      "    - ep_rew          : -109.33508507852815\n",
      "    - max_rew         : -78.1486177454997\n",
      "    - ind_max_rew     : 7\n",
      "\n",
      "- time/\n",
      "    - episodes        : 98\n",
      "    - fps             : 54\n",
      "    - time_elapsed    : 471\n",
      "    - total_timesteps : 25540\n",
      "\n",
      "- train/\n",
      "    - actor_loss      : -2.6338893431270396\n",
      "    - critic_loss1    : 0.12180646405190623\n",
      "    - critic_loss2    : 0.12180646405190623\n",
      "    - critic_loss3    : 0.12180646405190623\n",
      "    - critic_loss4    : 0.12180646405190623\n",
      "    - critic_loss5    : 0.12180646405190623\n",
      "    - learning_rate   : 0.0001\n",
      "    - n_updates       : 12642\n"
     ]
    }
   ],
   "source": [
    "scores = ddpg(env, agent, normalizer, reward_normalizer, N_TOTAL_EPISODES)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1066bda-dc92-477a-9944-b58443acf4de",
   "metadata": {},
   "source": [
    "### Plot rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe5527-b2a7-4b24-a25c-ccee897a002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the point history\n",
    "plot_history(scores, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0de69-f5a8-445e-bb47-5d0b5ec8853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the point Actor loss history\n",
    "plot_history(agent.actor_loss_history_mean, 100, y_label=\"Actor Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce5b25-ff52-4a50-bc13-8c7956494963",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(agent.n_critics):\n",
    "    plot_history(agent.critic_loss_history_mean[idx], 100, y_label=f\"Critic Loss {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5096fc4c-0359-4861-a411-0ef6773aaf33",
   "metadata": {},
   "source": [
    "## Visualizing\n",
    "### create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4260759-3660-440c-90ee-34d027a9d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"rgb_array\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "new_normalizer = Normalizer(state_size)\n",
    "new_normalizer.load()  # Load the saved statistics\n",
    "new_reward_normalizer = RewardNormalizer()\n",
    "new_reward_normalizer.load()\n",
    "agent2 = Agent(state_size, action_size, new_reward_normalizer, random_seed=0)\n",
    "agent2.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4679d-c3a6-462a-9f49-81205757ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_list = []\n",
    "scores = []\n",
    "best_score = -1500\n",
    "for i in range(10):\n",
    "    state, _ = env.reset()\n",
    "    #Select an action\n",
    "    normalized_state = new_normalizer.normalize(state)\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    episode_reward = 0\n",
    "    e_screen_list = []\n",
    "    while not done:\n",
    "        action = agent2.act(normalized_state, False).squeeze(0)\n",
    "    \n",
    "        # Printing env render (rgb_array)\n",
    "        screen = env.render()\n",
    "        # Add title to the screen\n",
    "        screen = cv2.putText(\n",
    "            np.array(screen),\n",
    "            f\"Iteration=[{i}] Timestep=[{timestep +1}]\",\n",
    "            (25, 25),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0, 0, 0),\n",
    "            1,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "        e_screen_list.append(screen)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = torch.from_numpy(next_state.T).float().squeeze(0).numpy()\n",
    "        done = terminated or truncated\n",
    "    \n",
    "        #Select an action\n",
    "        normalized_next_state = new_normalizer.normalize(next_state)\n",
    "        state = next_state\n",
    "        normalized_state = normalized_next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "        timestep += 1\n",
    "\n",
    "    if best_score <= episode_reward:\n",
    "        best_score = episode_reward\n",
    "        screen_list.append(e_screen_list)\n",
    "    \n",
    "    scores.append(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2bc83-9209-4f0b-b5fb-add05b51f782",
   "metadata": {},
   "source": [
    "### Scores of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5f20a-3240-44d4-bc69-9dc9d92679c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best score: {max(scores)}\")\n",
    "print(f\"AVG score: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b048482-0467-4d37-a2b2-4373536faf83",
   "metadata": {},
   "source": [
    "### Selection of the 5 best iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d31bb-0059-4937-a560-2297998f8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7027292-deb8-4d5b-97f0-b6d13b2ee90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_screen_list = screen_list[-n:]\n",
    "print(f\"Number of iter : {len(best_screen_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880138d-5a21-48df-8515-b08910659931",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_screens_list = np.empty((0, 400, 600, 3))\n",
    "for screen_ep in best_screen_list:\n",
    "    selected_screens_list = np.concatenate((selected_screens_list, np.array(screen_ep)), axis=0)\n",
    "\n",
    "selected_screens_list = selected_screens_list.astype(np.uint8)\n",
    "print(selected_screens_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c78e8-8329-4a24-9cbe-8efecce85feb",
   "metadata": {},
   "source": [
    "### Save gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5e5c7-2842-48e8-ae1b-9427fb38a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{PATH_IMG}.gif\"\n",
    "save_gif(list(selected_screens_list), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b76ee-24d9-4a92-90c1-90d2ca259625",
   "metadata": {},
   "source": [
    "### Embed the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f68a2-d370-4a65-87f2-21a06d9c81f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = open(path, 'rb').read()\n",
    "b64_video = base64.b64encode(video)\n",
    "video_tag = '<img src=\"data:image/gif;base64,{0}\">'.format(b64_video.decode())\n",
    "\n",
    "display(HTML(video_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34afa4f8-65cc-4e86-b670-0a5d46a56cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the point scores evaluation\n",
    "plot_history(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d696b-6aa9-4827-b0d5-af5119d8500a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe16936-08b3-4bff-ac8f-fa581d4a2dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d704a-e7f6-4df2-96a2-7d9e6f36a80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
