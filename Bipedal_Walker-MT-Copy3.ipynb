{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fb422d-6fb0-4159-b6e5-79bff41dc79b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bipedal Walker (easy)\n",
    "[Documentation](https://gymnasium.farama.org/environments/box2d/bipedal_walker/)  \n",
    "This environment is part of the Box2D environments which contains general information about the environment.\n",
    "\n",
    "[leaderboard](https://github.com/openai/gym/wiki/Leaderboard#bipedalwalker-v2)\n",
    "\n",
    "## Description\n",
    "This is a simple 4-joint walker robot environment. There are two versions:\n",
    "\n",
    "- Normal, with slightly uneven terrain.\n",
    "- Hardcore, with ladders, stumps, pitfalls.\n",
    "ls.\r\n",
    "\r\n",
    "To solve the normal version, you need to get 300 points in 1600 time steps. To solve the hardcore version, you need 300 points in 2000 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28409768-9bc1-44c3-82ed-0ca9e0b7f251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import os\n",
    "import base64\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# import third-party libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import imageio\n",
    "import cv2\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc43622-7836-473b-914d-6c143a7c5cb7",
   "metadata": {},
   "source": [
    "## Hardware infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fd67c7-c94a-49c2-90f7-e3ed1ea56ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1070 (UUID: GPU-19cef1c2-e216-e824-c98e-660394f8a4bb)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a786ed-9ed2-4f0b-ad4e-f1f22b4c0b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version.cuda=[11.7]\n",
      "torch.cuda.is_available(True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch.version.cuda=[{torch.version.cuda}]\")\n",
    "print(f\"torch.cuda.is_available({torch.cuda.is_available()})\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62e894b-412c-420a-bf10-7ede8eff4e55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232c832-cd45-432f-a602-4442a0105b43",
   "metadata": {},
   "source": [
    "## Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dc8a76-424c-4823-afb3-b04e7a116279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./img :\n",
      "total 29524\n",
      "drwxr-xr-x 1 thiba 197609        0 Jun 21 21:11 .\n",
      "drwxr-xr-x 1 thiba 197609        0 Jun 22 14:35 ..\n",
      "drwxr-xr-x 1 thiba 197609        0 Jun 21 21:11 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 thiba 197609 12930388 May  6 09:05 lunar_lander.gif\n",
      "-rw-r--r-- 1 thiba 197609 17291191 Jun 21 21:09 v3_bipedal_walker_easy.gif\n",
      "./saves :\n",
      "total 6780\n",
      "drwxr-xr-x 1 thiba 197609      0 Jun 22 11:43 .\n",
      "drwxr-xr-x 1 thiba 197609      0 Jun 22 14:35 ..\n",
      "drwxr-xr-x 1 thiba 197609      0 Jun 21 21:11 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 thiba 197609 457239 Jun 22 14:33 bipedal_walker_easy_actor_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 882535 Jun 22 14:33 bipedal_walker_easy_critic0_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 882535 Jun 22 14:33 bipedal_walker_easy_critic1_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 882535 Jun 22 14:33 bipedal_walker_easy_critic2_model.pth\n",
      "-rw-r--r-- 1 thiba 197609   1038 Jun 22 14:33 bipedal_walker_easy_normalizer.pkl\n",
      "-rw-r--r-- 1 thiba 197609 846941 Jun 21 20:38 v3_bipedal_walker_easy_actor_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 847985 Jun 21 20:38 v3_bipedal_walker_easy_critic0_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 847985 Jun 21 20:38 v3_bipedal_walker_easy_critic1_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 847985 Jun 21 20:38 v3_bipedal_walker_easy_critic2_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 204451 Jun 21 16:04 v3_bipedal_walker_easy_critic3_model.pth\n",
      "-rw-r--r-- 1 thiba 197609 204451 Jun 21 16:04 v3_bipedal_walker_easy_critic4_model.pth\n",
      "-rw-r--r-- 1 thiba 197609   1038 Jun 21 20:38 v3_bipedal_walker_easy_normalizer.pkl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('img', exist_ok=True)\n",
    "os.makedirs('saves', exist_ok=True)\n",
    "print('./img :')\n",
    "!ls -al img\n",
    "print('./saves :')\n",
    "!ls -al saves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc8f7e-4ed4-4a9e-b222-f36d75a8efdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constants\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c447b469-f41b-48a4-98fd-bea9449be473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1_000_000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.98\n",
    "TAU = 0.02\n",
    "LR_ACTOR = 0.0001\n",
    "LR_CRITIC = 0.0003\n",
    "WEIGHT_DECAY = 0.00\n",
    "POLICY_NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "POLICY_FREQ = 2\n",
    "START_LEARNING = 10000 # steps in the beginning to let the agent explore\n",
    "N_TOTAL_EPISODES = 1500\n",
    "N_CRITICS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fece19-c43a-45aa-95b1-b8a1a42ede8c",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c6b75d-0fe2-4209-ba7c-ceb424985e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./saves/bipedal_walker_easy\"\n",
    "PATH_IMG = \"./img/bipedal_walker_easy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2e836-6d3c-411e-a629-f71c7ee5df96",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a31f47-dea9-4943-affa-2c3463dd42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_infos(ep_len_mean, ep_rew_mean, ep_len, ep_rew, min_rew, ep_min_rew, episodes, fps, time_elapsed, total_timesteps, actor_loss, critic_loss_arr, learning_rate, n_updates):\n",
    "    print(f\"- rollout/\")\n",
    "    print(f\"    - ep_len_mean     : {ep_len_mean}\")\n",
    "    print(f\"    - ep_rew_mean     : {ep_rew_mean}\")\n",
    "    print(f\"    - ep_len          : {ep_len}\")\n",
    "    print(f\"    - ep_rew          : {ep_rew}\")\n",
    "    print(f\"    - max_rew         : {min_rew}\")\n",
    "    print(f\"    - ind_max_rew     : {ep_min_rew}\")\n",
    "    print(f\"\")\n",
    "    print(f\"- time/\")\n",
    "    print(f\"    - episodes        : {episodes}\")\n",
    "    print(f\"    - fps             : {fps}\")\n",
    "    print(f\"    - time_elapsed    : {time_elapsed}\")\n",
    "    print(f\"    - total_timesteps : {total_timesteps}\")\n",
    "    print(f\"\")\n",
    "    print(f\"- train/\")\n",
    "    print(f\"    - actor_loss      : {actor_loss}\")\n",
    "    for i, c in enumerate(critic_loss_arr):\n",
    "        print(f\"    - critic_loss{i+1}    : {c}\")\n",
    "    print(f\"    - learning_rate   : {learning_rate}\")\n",
    "    print(f\"    - n_updates       : {n_updates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f0e68e6-212f-4df7-b36b-fe425b1a0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gif(img_list, path):\n",
    "    # Convert the list of frames to a numpy array\n",
    "    resized_img_array = []\n",
    "    for img in img_list:\n",
    "        img_pil = Image.fromarray(img)\n",
    "        # Make sure width and height are divisible by 16\n",
    "        img_resized_pil = img_pil.resize((608, 400))\n",
    "        img_resized = np.array(img_resized_pil)\n",
    "        resized_img_array.append(img_resized)\n",
    "    \n",
    "    # Create gif video\n",
    "    fps = 20\n",
    "    imageio.mimsave(path, resized_img_array, 'GIF', duration=int(1000 * 1/fps), loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b22232-a375-4c23-ba54-c7a818c19eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(reward_history, rolling_window=20, x_label = 'Episode', y_label = 'Total Points', lower_limit=None, upper_limit=None, plot_rw=True, plot_rm=True):\n",
    "    \"\"\"\n",
    "    Function to plot reward history and its rolling mean with some optional arguments.\n",
    "\n",
    "    Args:\n",
    "        reward_history (list): A list of rewards for each episode.\n",
    "        rolling_window (int): The number of episodes for computing the rolling mean.\n",
    "        lower_limit (int): Starting episode index for plotting.\n",
    "        upper_limit (int): Ending episode index for plotting.\n",
    "        plot_rw (bool): A flag for plotting raw reward history.\n",
    "        plot_rm (bool): A flag for plotting rolling mean reward history.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # If lower_limit and upper_limit are not provided, use the whole reward_history\n",
    "    if lower_limit is None or upper_limit is None:\n",
    "        rh = reward_history\n",
    "        xs = [x for x in range(len(reward_history))]\n",
    "    else:\n",
    "        rh = reward_history[lower_limit:upper_limit]\n",
    "        xs = [x for x in range(lower_limit,upper_limit)]\n",
    "   \n",
    "    # Create a DataFrame and calculate the rolling mean\n",
    "    df = pd.DataFrame(rh)\n",
    "    rollingMean = df.rolling(rolling_window).mean()\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10,7), facecolor='white')\n",
    "    \n",
    "    if plot_rw:\n",
    "        plt.plot(xs, rh, linewidth=1, color='cyan')\n",
    "    if plot_rm:\n",
    "        plt.plot(xs, rollingMean, linewidth=2, color='magenta')\n",
    "\n",
    "    text_color = 'black'\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('black')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.xlabel(x_label, color=text_color, fontsize=30)\n",
    "    plt.ylabel(y_label, color=text_color, fontsize=30)\n",
    "    yNumFmt = mticker.StrMethodFormatter('{x:,}')\n",
    "    ax.yaxis.set_major_formatter(yNumFmt)\n",
    "    ax.tick_params(axis='x', colors=text_color)\n",
    "    ax.tick_params(axis='y', colors=text_color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa23f826-b9fd-4a24-bc04-f587bd7c4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hyperparameters():\n",
    "    print(\"----------------------------------\")\n",
    "    print(f\"| {BUFFER_SIZE=} \\t\\t |\")\n",
    "    print(f\"| {BATCH_SIZE=} \\t\\t |\")\n",
    "    print(f\"| {GAMMA=} \\t\\t\\t |\")\n",
    "    print(f\"| {TAU=} \\t\\t\\t |\")\n",
    "    print(f\"| {LR_ACTOR=} \\t\\t |\")\n",
    "    print(f\"| {LR_CRITIC=} \\t\\t |\")\n",
    "    print(f\"| {WEIGHT_DECAY=} \\t\\t |\")\n",
    "    print(f\"| {POLICY_NOISE=} \\t\\t |\")\n",
    "    print(f\"| {NOISE_CLIP=} \\t\\t |\")\n",
    "    print(f\"| {POLICY_FREQ=} \\t\\t |\")\n",
    "    print(f\"| {START_LEARNING=} \\t\\t |\")\n",
    "    print(f\"| {N_TOTAL_EPISODES=} \\t |\")\n",
    "    print(f\"| {N_CRITICS=} \\t\\t\\t |\")\n",
    "    print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d16eb8-5c5f-4aed-9bdb-b3e6724da8c9",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb680d6e-63ed-46d6-a891-cf2588682ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a453c0e-37c9-4aea-b294-30527387c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838f137-bd48-4670-b25c-cc0e90330274",
   "metadata": {},
   "source": [
    "## Normalizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5fd11-8760-43f8-be80-bc31e05be9ed",
   "metadata": {},
   "source": [
    "### States normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f176383e-c8be-407e-be90-763585bb9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    def __init__(self, size):\n",
    "        self.n = np.zeros(size)\n",
    "        self.mean = np.zeros(size)\n",
    "        self.mean_diff = np.zeros(size)\n",
    "        self.var = np.zeros(size)\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1.0\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2) \n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std\n",
    "\n",
    "    def observe2d(self, x):\n",
    "        x = np.atleast_2d(x)  # Make sure x is at least 2-D\n",
    "        batch_size, _ = x.shape\n",
    "        self.n += batch_size\n",
    "        for instance in x:\n",
    "            last_mean = self.mean.copy()\n",
    "            self.mean += (instance - self.mean) / self.n\n",
    "            self.mean_diff += (instance - last_mean) * (instance - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min=1e-2)\n",
    "\n",
    "    def save(self):\n",
    "        with open(f\"{PATH}_normalizer.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "\n",
    "    def load(self):\n",
    "        with open(f\"{PATH}_normalizer.pkl\", 'rb') as f:\n",
    "            tmp_dict = pickle.load(f)\n",
    "        self.__dict__.update(tmp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05fbb91-a958-4102-9a5b-4d77b46dc0ff",
   "metadata": {},
   "source": [
    "### Reward normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "190de777-373f-4fe7-9d8e-cf41e1003fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNormalizer():\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.mean = 0\n",
    "        self.mean_diff = 0\n",
    "        self.var = 1e-2  # Start with small variance to avoid division by zero\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1\n",
    "        last_mean = self.mean\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2) \n",
    "\n",
    "    def normalize(self, reward):\n",
    "        reward_std = np.sqrt(self.var)\n",
    "        return (reward - self.mean) / reward_std\n",
    "\n",
    "    def observe2d(self, x):\n",
    "        x = np.atleast_2d(x)  # Make sure x is at least 2-D\n",
    "        batch_size, _ = x.shape\n",
    "        self.n += batch_size\n",
    "        for instance in x:\n",
    "            last_mean = self.mean.copy()\n",
    "            self.mean += (instance - self.mean) / self.n\n",
    "            self.mean_diff += (instance - last_mean) * (instance - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min=1e-2)\n",
    "\n",
    "    def save(self):\n",
    "        with open(f\"{PATH}_reward_normalizer.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "\n",
    "    def load(self):\n",
    "        with open(f\"{PATH}_reward_normalizer.pkl\", 'rb') as f:\n",
    "            tmp_dict = pickle.load(f)\n",
    "        self.__dict__.update(tmp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17891fa-d532-4640-84da-92703b5d5a37",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d0fb2-6a15-47d0-b950-cda0633cd3d5",
   "metadata": {},
   "source": [
    "### Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9310d32-699c-45a5-a79c-39dde680481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae238fd-51ce-4f03-a50c-f7ea14c77301",
   "metadata": {},
   "source": [
    "### Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "103e166c-360e-411c-a3f7-b2e63a25247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=512, fc2_units=400):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e547ed-4f0a-4818-aab6-b67539bd64b1",
   "metadata": {},
   "source": [
    "## DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de92d82-9a80-4a3d-b40a-b558897f0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, reward_normalizer, n_critics = 3, random_seed = 0):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network \n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        self.actor_loss_history = []\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.n_critics = n_critics\n",
    "        self.critic_local = [Critic(state_size, action_size, random_seed).to(device) for _ in range(self.n_critics)]\n",
    "        self.critic_target = [Critic(state_size, action_size, random_seed).to(device) for _ in range(self.n_critics)]\n",
    "        self.critic_optimizer = [optim.Adam(self.critic_local[idx].parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY) for idx in range(self.n_critics)]\n",
    "        self.critic_loss_history = [[] for _ in range(self.n_critics)]\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "        # Reward Normalizer\n",
    "        self.reward_normalizer = reward_normalizer\n",
    "        \n",
    "        self.n_updates = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done, timestep):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % POLICY_FREQ == 0:\n",
    "            experiences = self.memory.sample()\n",
    "            states, actions, rewards, next_states, dones = experiences\n",
    "            # switch to numpy\n",
    "            states = states.cpu().numpy()\n",
    "            next_states = next_states.cpu().numpy()\n",
    "            rewards = rewards.cpu().numpy()\n",
    "            # Normalize states\n",
    "            normalizer.observe2d(states)\n",
    "            states = normalizer.normalize(states)\n",
    "            # Normalize next_states\n",
    "            normalizer.observe2d(next_states)\n",
    "            next_states = normalizer.normalize(next_states)\n",
    "            # Normalize rewards\n",
    "            self.reward_normalizer.observe(rewards)\n",
    "            rewards = self.reward_normalizer.normalize(rewards)\n",
    "            # switch to tensor\n",
    "            states = torch.from_numpy(states).float().to(device)\n",
    "            next_states = torch.from_numpy(next_states).float().to(device)\n",
    "            rewards = torch.from_numpy(rewards).float().to(device)\n",
    "            self.learn((states, actions, rewards, next_states, dones), GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            noise = np.random.normal(0, POLICY_NOISE, size=self.action_size)\n",
    "            noise = np.clip(noise, -NOISE_CLIP, NOISE_CLIP)\n",
    "            action += noise\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.actor_local.state_dict(), f\"{PATH}_actor_model.pth\")\n",
    "        for idx, critic_local in enumerate(self.critic_local):\n",
    "            torch.save(critic_local.state_dict(), f\"{PATH}_critic{idx}_model.pth\")\n",
    "\n",
    "    def load_model(self):\n",
    "        self.actor_local.load_state_dict(torch.load(f\"{PATH}_actor_model.pth\"))\n",
    "        for idx in range(self.n_critics):\n",
    "            self.critic_local[idx].load_state_dict(torch.load(f\"{PATH}_critic{idx}_model.pth\"))\n",
    "        \n",
    "    def get_lr(self):\n",
    "        for param_group in self.actor_optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        noise = torch.normal(torch.zeros(actions.size()), POLICY_NOISE).to(device)\n",
    "        noise = torch.clamp(noise, -NOISE_CLIP, NOISE_CLIP)\n",
    "        actions_next = self.actor_target(next_states) + noise\n",
    "        actions_next = torch.clamp(actions_next, -1, 1)\n",
    "\n",
    "        Q_targets_next_arr = [self.critic_target[idx](next_states, actions_next) for idx in range(self.n_critics)]\n",
    "\n",
    "        Q_targets_next = Q_targets_next_arr[0]\n",
    "        for idx in range(1, self.n_critics):\n",
    "            Q_targets_next = torch.min(Q_targets_next, Q_targets_next_arr[idx])\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Compute all critic loss\n",
    "        for idx in range(self.n_critics):\n",
    "            Q_expected = self.critic_local[idx](states, actions)\n",
    "            critic_loss = F.mse_loss(Q_expected, Q_targets.detach()) # detach Q_targets here to avoid the \"the graph are freed\" error\n",
    "            # Save the critic loss\n",
    "            self.critic_loss_history[idx].append(critic_loss.item())\n",
    "            # Update critic loss\n",
    "            self.critic_optimizer[idx].zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer[idx].step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        actions_pred = self.actor_local(states)\n",
    "        # Compute the average of the critics\n",
    "        critic_values = [self.critic_local[idx](states, actions_pred) for idx in range(self.n_critics)]\n",
    "        critic_value = sum(critic_values) / float(self.n_critics)\n",
    "        actor_loss = -critic_value.mean()\n",
    "        # Save the loss\n",
    "        self.actor_loss_history.append(actor_loss.item())\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        for idx in range(self.n_critics):\n",
    "            self.soft_update(self.critic_local[idx], self.critic_target[idx], TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        \n",
    "        self.n_updates += 1\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8bccf-b22e-41c3-ba08-8043bd439e2a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9e8e6f-725d-4078-b859-5707afafc212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(env, agent, normalizer, n_episodes=1000, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    timesteps_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    start_time_elapsed = time.time()\n",
    "    total_timesteps = 0\n",
    "    min_rew = -1500 \n",
    "    ep_min_rew = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # function to override printlines from previous loop iteration \n",
    "        clear_output(wait=True)\n",
    "        state, _ = env.reset()\n",
    "        # State normalization\n",
    "        normalizer.observe(state)\n",
    "        normalized_state = normalizer.normalize(state)\n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            if total_timesteps >= START_LEARNING:\n",
    "                action = agent.act(normalized_state).squeeze(0)\n",
    "            else:\n",
    "                action = env.action_space.sample()  # Sample random action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = torch.from_numpy(next_state.T).float().squeeze(0).numpy()\n",
    "            \n",
    "            # State normalization\n",
    "            normalizer.observe(next_state)\n",
    "            normalized_next_state = normalizer.normalize(next_state)\n",
    "        \n",
    "            done = terminated or truncated\n",
    "            agent.step(state, action, reward, next_state, done, total_timesteps)\n",
    "            state = next_state\n",
    "            normalized_state = normalized_next_state\n",
    "            score += reward\n",
    "            timestep += 1\n",
    "            total_timesteps += 1\n",
    "        scores_deque.append(score)\n",
    "        timesteps_deque.append(timestep)\n",
    "        scores.append(score)\n",
    "\n",
    "        time_elapsed = time.time() - start_time_elapsed\n",
    "        if min_rew <= score:\n",
    "            min_rew = score\n",
    "            ep_min_rew = i_episode\n",
    "            agent.save_model()\n",
    "            normalizer.save()  \n",
    "        print_infos(\n",
    "            int(np.mean(timesteps_deque)),\n",
    "            np.mean(scores_deque),\n",
    "            timestep,\n",
    "            score,\n",
    "            min_rew,\n",
    "            ep_min_rew, \n",
    "            i_episode,\n",
    "            int(total_timesteps / time_elapsed),\n",
    "            int(time_elapsed),\n",
    "            total_timesteps,\n",
    "            np.mean(agent.actor_loss_history),\n",
    "            [np.mean(critic_loss_history) for critic_loss_history in agent.critic_loss_history],\n",
    "            agent.get_lr(),\n",
    "            agent.n_updates\n",
    "        )\n",
    "\n",
    "        if np.mean(scores_deque) >= 300:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321dd5b3-6430-4854-8ed1-0566dd76e768",
   "metadata": {},
   "source": [
    "### create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00717104-4277-4c22-b74a-f47d38c62f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| BUFFER_SIZE=1000000 \t\t |\n",
      "| BATCH_SIZE=128 \t\t |\n",
      "| GAMMA=0.98 \t\t\t |\n",
      "| TAU=0.02 \t\t\t |\n",
      "| LR_ACTOR=0.0001 \t\t |\n",
      "| LR_CRITIC=0.0003 \t\t |\n",
      "| WEIGHT_DECAY=0.0 \t\t |\n",
      "| POLICY_NOISE=0.2 \t\t |\n",
      "| NOISE_CLIP=0.5 \t\t |\n",
      "| POLICY_FREQ=2 \t\t |\n",
      "| START_LEARNING=10000 \t\t |\n",
      "| N_TOTAL_EPISODES=1500 \t |\n",
      "| N_CRITICS=3 \t\t\t |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218e8cb-dec0-4b9c-9d18-48a1cf2f4416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- rollout/\n",
      "    - ep_len_mean     : 892\n",
      "    - ep_rew_mean     : -113.48731746943828\n",
      "    - ep_len          : 357\n",
      "    - ep_rew          : -126.18929845913571\n",
      "    - max_rew         : -28.98261458703377\n",
      "    - ind_max_rew     : 172\n",
      "\n",
      "- time/\n",
      "    - episodes        : 186\n",
      "    - fps             : 101\n",
      "    - time_elapsed    : 1195\n",
      "    - total_timesteps : 120957\n",
      "\n",
      "- train/\n",
      "    - actor_loss      : -5.843022902518804\n",
      "    - critic_loss1    : 0.112986796827377\n",
      "    - critic_loss2    : 0.112986796827377\n",
      "    - critic_loss3    : 0.112986796827377\n",
      "    - learning_rate   : 0.0001\n",
      "    - n_updates       : 60415\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=False)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "normalizer = Normalizer(state_size)\n",
    "reward_normalizer = RewardNormalizer()\n",
    "agent = Agent(state_size, action_size, reward_normalizer, N_CRITICS, random_seed=0)\n",
    "scores = ddpg(env, agent, normalizer, N_TOTAL_EPISODES)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1066bda-dc92-477a-9944-b58443acf4de",
   "metadata": {},
   "source": [
    "### Plot rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe5527-b2a7-4b24-a25c-ccee897a002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the point history\n",
    "plot_history(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0de69-f5a8-445e-bb47-5d0b5ec8853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the point Actor loss history\n",
    "# Reshape the data to have N_TOTAL_EPISODES number of chunks\n",
    "actor_loss_history = np.array(agent.actor_loss_history)[:190000].reshape(N_TOTAL_EPISODES, -1)\n",
    "# Compute the mean along axis 1 (which represents each chunk of 200 points)\n",
    "mean_actor_loss_history = actor_loss_history.mean(axis=1)\n",
    "plot_history(mean_actor_loss_history, y_label=\"Actor Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce5b25-ff52-4a50-bc13-8c7956494963",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(agent.n_critics):\n",
    "    critic_loss_history = np.array(agent.critic_loss_history[idx])[:190000].reshape(N_TOTAL_EPISODES, -1)\n",
    "    # Compute the mean along axis 1 (which represents each chunk of 200 points)\n",
    "    mean_critic_loss_history = critic_loss_history.mean(axis=1)\n",
    "    plot_history(mean_critic_loss_history, y_label=f\"Critic Loss {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5096fc4c-0359-4861-a411-0ef6773aaf33",
   "metadata": {},
   "source": [
    "## Visualizing\n",
    "### create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4260759-3660-440c-90ee-34d027a9d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=False, render_mode=\"rgb_array\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "new_normalizer = Normalizer(state_size)\n",
    "new_normalizer.load()  # Load the saved statistics\n",
    "agent2 = Agent(state_size, action_size, random_seed=0)\n",
    "agent2.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4679d-c3a6-462a-9f49-81205757ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_list = []\n",
    "scores = []\n",
    "best_score = -1500\n",
    "for i in range(10):\n",
    "    state, _ = env.reset()\n",
    "    #Select an action\n",
    "    normalized_state = normalizer.normalize(state)\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    episode_reward = 0\n",
    "    e_screen_list = []\n",
    "    while not done:\n",
    "        action = agent2.act(normalized_state, False).squeeze(0)\n",
    "    \n",
    "        # Printing env render (rgb_array)\n",
    "        screen = env.render()\n",
    "        # Add title to the screen\n",
    "        screen = cv2.putText(\n",
    "            np.array(screen),\n",
    "            f\"Iteration=[{i}] Timestep=[{timestep +1}]\",\n",
    "            (25, 25),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0, 0, 0),\n",
    "            1,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "        e_screen_list.append(screen)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = torch.from_numpy(next_state.T).float().squeeze(0).numpy()\n",
    "        done = terminated or truncated\n",
    "    \n",
    "        #Select an action\n",
    "        normalized_next_state = normalizer.normalize(next_state)\n",
    "        state = next_state\n",
    "        normalized_state = normalized_next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "        timestep += 1\n",
    "\n",
    "    if best_score <= episode_reward:\n",
    "        best_score = episode_reward\n",
    "        screen_list.append(e_screen_list)\n",
    "    \n",
    "    scores.append(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2bc83-9209-4f0b-b5fb-add05b51f782",
   "metadata": {},
   "source": [
    "### Scores of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5f20a-3240-44d4-bc69-9dc9d92679c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best score: {max(scores)}\")\n",
    "print(f\"AVG score: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b048482-0467-4d37-a2b2-4373536faf83",
   "metadata": {},
   "source": [
    "### Selection of the 5 best iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d31bb-0059-4937-a560-2297998f8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7027292-deb8-4d5b-97f0-b6d13b2ee90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_screen_list = screen_list[-n:]\n",
    "print(f\"Number of iter : {len(best_screen_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880138d-5a21-48df-8515-b08910659931",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_screens_list = np.empty((0, 400, 600, 3))\n",
    "for screen_ep in best_screen_list:\n",
    "    selected_screens_list = np.concatenate((selected_screens_list, np.array(screen_ep)), axis=0)\n",
    "\n",
    "selected_screens_list = selected_screens_list.astype(np.uint8)\n",
    "print(selected_screens_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c78e8-8329-4a24-9cbe-8efecce85feb",
   "metadata": {},
   "source": [
    "### Save gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5e5c7-2842-48e8-ae1b-9427fb38a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{PATH_IMG}.gif\"\n",
    "save_gif(list(selected_screens_list), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b76ee-24d9-4a92-90c1-90d2ca259625",
   "metadata": {},
   "source": [
    "### Embed the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f68a2-d370-4a65-87f2-21a06d9c81f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = open(path, 'rb').read()\n",
    "b64_video = base64.b64encode(video)\n",
    "video_tag = '<img src=\"data:image/gif;base64,{0}\">'.format(b64_video.decode())\n",
    "\n",
    "display(HTML(video_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34afa4f8-65cc-4e86-b670-0a5d46a56cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the point scores evaluation\n",
    "plot_history(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d696b-6aa9-4827-b0d5-af5119d8500a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe16936-08b3-4bff-ac8f-fa581d4a2dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d704a-e7f6-4df2-96a2-7d9e6f36a80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
